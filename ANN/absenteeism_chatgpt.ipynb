{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34585f8c-250c-4ec8-afc3-c66f78c3fcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# 1. Define the updated dataset\n",
    "data = {\n",
    "    'date': ['2023-11-01', '2023-11-02', '2023-11-03', '2023-11-04'],  # Date column\n",
    "    'reason': [0, 1, 2, 1],  # Categorical but numeric\n",
    "    'Age': [25, 30, 35, 30],  # Continuous numeric\n",
    "    'Distance to Work': [10.0, 20.0, 15.0, 25.0],  # Continuous numeric\n",
    "    'Transportation Expense': [200, 250, 300, 220],  # Continuous numeric\n",
    "    'Absenteeism Time in Hours': [5, 8, 6, 7]  # Target variable (y)\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Ensure 'date' is in datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Set the target variable\n",
    "y = df['Absenteeism Time in Hours'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7324fc0-508f-4f76-879f-0b10c2544d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'date' since itâ€™s not numeric or directly useful for modeling\n",
    "df = df.drop(columns=['date'])\n",
    "\n",
    "# Define preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), ['reason']),  # OneHotEncode 'reason'\n",
    "        ('num', StandardScaler(), ['Age', 'Distance to Work', 'Transportation Expense'])  # Scale numeric columns\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Transform the features\n",
    "X_processed = preprocessor.fit_transform(df.drop(columns=['Absenteeism Time in Hours']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45884218-4eb1-4d34-8e5a-93204e16ffa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Define the ANN model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer with the size of processed features\n",
    "model.add(Dense(units=16, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "\n",
    "# Hidden layers\n",
    "model.add(Dense(units=8, activation='relu'))\n",
    "model.add(Dense(units=4, activation='relu'))\n",
    "\n",
    "# Output layer for regression\n",
    "model.add(Dense(units=1, activation='linear'))  # Linear activation for regression\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])  # Use mean squared error for regression\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=4, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {loss}, Test MAE: {mae}\")\n",
    "\n",
    "# Predict on test data\n",
    "predictions = model.predict(X_test)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faee0382-f089-4418-aa17-07afe27b37c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Example dataset\n",
    "data = {\n",
    "    'date': ['2023-11-01', '2023-11-02', '2023-11-03', '2023-11-04'],\n",
    "    'reason': [0, 1, 2, 28],  # Reason column with values 0 to 28\n",
    "    'Age': [25, 30, 35, 30],\n",
    "    'Distance to Work': [10.0, 20.0, 15.0, 25.0],\n",
    "    'Transportation Expense': [200, 250, 300, 220],\n",
    "    'Absenteeism Time in Hours': [5, 8, 6, 7]  # Target variable\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert 'date' to datetime format (just for completeness)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Target variable\n",
    "y = df['Absenteeism Time in Hours']\n",
    "\n",
    "# Drop the 'date' and target column\n",
    "X = df.drop(columns=['date', 'Absenteeism Time in Hours'])\n",
    "\n",
    "# Group the 'reason' values into custom groups using a lambda function\n",
    "def group_reasons(reason):\n",
    "    if 0 <= reason <= 4:\n",
    "        return 0  # Group 1\n",
    "    elif 5 <= reason <= 9:\n",
    "        return 1  # Group 2\n",
    "    elif 10 <= reason <= 14:\n",
    "        return 2  # Group 3\n",
    "    elif 15 <= reason <= 19:\n",
    "        return 3  # Group 4\n",
    "    elif 20 <= reason <= 24:\n",
    "        return 4  # Group 5\n",
    "    else:\n",
    "        return 5  # Group 6 (for 25-28)\n",
    "\n",
    "# Apply the grouping function to the 'reason' column\n",
    "X['reason_grouped'] = X['reason'].apply(group_reasons)\n",
    "\n",
    "# Drop the original 'reason' column as it is now grouped\n",
    "X = X.drop(columns=['reason'])\n",
    "\n",
    "# Define the preprocessor pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), ['reason_grouped']),  # One-hot encode the grouped 'reason'\n",
    "        ('num', StandardScaler(), ['Age', 'Distance to Work', 'Transportation Expense'])  # Scale numeric columns\n",
    "    ])\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer size matches the number of processed features\n",
    "model.add(Dense(units=16, activation='relu', input_shape=(X.shape[1],)))\n",
    "\n",
    "# Hidden layers\n",
    "model.add(Dense(units=8, activation='relu'))\n",
    "model.add(Dense(units=4, activation='relu'))\n",
    "\n",
    "# Output layer for regression\n",
    "model.add(Dense(units=1, activation='linear'))  # Use linear activation for regression\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Create a full pipeline that includes the preprocessor and the model\n",
    "full_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),  # Apply preprocessing\n",
    "    ('model', model)  # Fit the model\n",
    "])\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "history = full_pipeline.fit(X_train, y_train, epochs=50, batch_size=4, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {loss}, Test MAE: {mae}\")\n",
    "\n",
    "# Predict on the test data\n",
    "predictions = model.predict(X_test)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py_tf)",
   "language": "python",
   "name": "py_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
